# Usage
#   Start:              docker compose up
#   With helpers:       docker compose -f docker-compose.yml -f ./dev/docker-compose.dev.yml up
#   Stop:               docker compose down
#   Destroy:            docker compose -f docker-compose.yml -f ./dev/docker-compose.dev.yml down -v --remove-orphans
#   Reset everything:  ./reset.sh

networks:
  ecomweb_external_net:
    external: true
  ecomweb_internal_net:
    external: true

name: supabase

services:

  studio:
    container_name: supabase-studio
    image: supabase/studio:2025.06.30-sha-6f5982d
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "node",
          "-e",
          "fetch('http://studio:3000/api/platform/profile').then((r) => {if (r.status !== 200) throw new Error(r.status)})"
        ]
      timeout: 10s
      interval: 5s
      retries: 3
    depends_on:
      analytics:
        condition: service_healthy
    environment:
      STUDIO_PG_META_URL: http://meta:8080
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}

      DEFAULT_ORGANIZATION_NAME: ${STUDIO_DEFAULT_ORGANIZATION}
      DEFAULT_PROJECT_NAME: ${STUDIO_DEFAULT_PROJECT}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}

      SUPABASE_URL: http://kong:8000
      SUPABASE_PUBLIC_URL: ${SUPABASE_PUBLIC_URL}
      SUPABASE_ANON_KEY: ${ANON_KEY}
      SUPABASE_SERVICE_KEY: ${SERVICE_ROLE_KEY}
      AUTH_JWT_SECRET: ${JWT_SECRET}

      LOGFLARE_PRIVATE_ACCESS_TOKEN: ${LOGFLARE_PRIVATE_ACCESS_TOKEN}
      LOGFLARE_URL: http://analytics:4000
      NEXT_PUBLIC_ENABLE_LOGS: true
      # Comment to use Big Query backend for analytics
      # NEXT_ANALYTICS_BACKEND_PROVIDER: postgres
      # Uncomment to use Big Query backend for analytics
      NEXT_ANALYTICS_BACKEND_PROVIDER: bigquery

    networks:
      - ecomweb_internal_net

  ecomweb_haproxy:
    image: haproxy:latest
    container_name: ecomweb_haproxy
    ports:
      - "80"      # Main API port
      - "10.7.0.4:8404:8404"    # Stats page
    volumes:
      - ./volumes/haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
      - ./volumes/haproxy/errors:/etc/haproxy/errorfiles
    networks:
      - ecomweb_internal_net
    restart: always
    sysctls:
      - net.ipv4.ip_unprivileged_port_start=0
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:80/haproxy-health"]
      interval: 10s
      timeout: 5s
      retries: 3    

  kong:
    container_name: supabase-kong
    image: kong:2.8.1
    restart: unless-stopped
    ports:
      - ${KONG_HTTP_PORT}:8000/tcp
      - ${KONG_HTTPS_PORT}:8443/tcp
    volumes:
      # https://github.com/supabase/supabase/issues/12661
      - ./volumes/api/kong.yml:/home/kong/temp.yml:ro,z
    depends_on:
      analytics:
        condition: service_healthy
    environment:
      KONG_DATABASE: "off"
      KONG_DECLARATIVE_CONFIG: /home/kong/kong.yml
      # https://github.com/supabase/cli/issues/14
      KONG_DNS_ORDER: LAST,A,CNAME
      KONG_PLUGINS: request-transformer,cors,key-auth,acl,basic-auth
      KONG_NGINX_PROXY_PROXY_BUFFER_SIZE: 160k
      KONG_NGINX_PROXY_PROXY_BUFFERS: 64 160k
      SUPABASE_ANON_KEY: ${ANON_KEY}
      SUPABASE_SERVICE_KEY: ${SERVICE_ROLE_KEY}
      DASHBOARD_USERNAME: ${DASHBOARD_USERNAME}
      DASHBOARD_PASSWORD: ${DASHBOARD_PASSWORD}
    # https://unix.stackexchange.com/a/294837
    entrypoint: bash -c 'eval "echo \"$$(cat ~/temp.yml)\"" > ~/kong.yml && /docker-entrypoint.sh kong docker-start'

    networks:
      - ecomweb_internal_net
      - ecomweb_external_net

  auth:
    container_name: supabase-auth
    image: supabase/gotrue:v2.177.0
    restart: unless-stopped
    dns:
      - 8.8.8.8
      - 1.1.1.1
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://localhost:9999/health"
        ]
      timeout: 5s
      interval: 5s
      retries: 3
    depends_on:
      db:
        # Disable this if you are using an external Postgres database
        condition: service_healthy
      analytics:
        condition: service_healthy
    environment:
      GOTRUE_API_HOST: 0.0.0.0
      GOTRUE_API_PORT: 9999
      API_EXTERNAL_URL: ${API_EXTERNAL_URL}

      GOTRUE_DB_DRIVER: postgres
      GOTRUE_DB_DATABASE_URL: postgres://supabase_auth_admin:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}

      GOTRUE_SITE_URL: ${SITE_URL}
      GOTRUE_URI_ALLOW_LIST: ${ADDITIONAL_REDIRECT_URLS}
      GOTRUE_DISABLE_SIGNUP: ${DISABLE_SIGNUP}

      GOTRUE_JWT_ADMIN_ROLES: service_role
      GOTRUE_JWT_AUD: authenticated
      GOTRUE_JWT_DEFAULT_GROUP_NAME: authenticated
      GOTRUE_JWT_EXP: ${JWT_EXPIRY}
      GOTRUE_JWT_SECRET: ${JWT_SECRET}

      GOTRUE_EXTERNAL_EMAIL_ENABLED: ${ENABLE_EMAIL_SIGNUP}
      GOTRUE_EXTERNAL_ANONYMOUS_USERS_ENABLED: ${ENABLE_ANONYMOUS_USERS}
      GOTRUE_MAILER_AUTOCONFIRM: ${ENABLE_EMAIL_AUTOCONFIRM}

      # Uncomment to bypass nonce check in ID Token flow. Commonly set to true when using Google Sign In on mobile.
      GOTRUE_EXTERNAL_SKIP_NONCE_CHECK: true

      # GOTRUE_MAILER_SECURE_EMAIL_CHANGE_ENABLED: true
      # GOTRUE_SMTP_MAX_FREQUENCY: 1s
      GOTRUE_SMTP_ADMIN_EMAIL: ${SMTP_ADMIN_EMAIL}
      GOTRUE_SMTP_HOST: ${SMTP_HOST}
      GOTRUE_SMTP_PORT: ${SMTP_PORT}
      GOTRUE_SMTP_USER: ${SMTP_USER}
      GOTRUE_SMTP_PASS: ${SMTP_PASS}
      GOTRUE_SMTP_SENDER_NAME: ${SMTP_SENDER_NAME}
      GOTRUE_MAILER_URLPATHS_INVITE: ${MAILER_URLPATHS_INVITE}
      GOTRUE_MAILER_URLPATHS_CONFIRMATION: ${MAILER_URLPATHS_CONFIRMATION}
      GOTRUE_MAILER_URLPATHS_RECOVERY: ${MAILER_URLPATHS_RECOVERY}
      GOTRUE_MAILER_URLPATHS_EMAIL_CHANGE: ${MAILER_URLPATHS_EMAIL_CHANGE}

      GOTRUE_EXTERNAL_PHONE_ENABLED: ${ENABLE_PHONE_SIGNUP}
      GOTRUE_SMS_AUTOCONFIRM: ${ENABLE_PHONE_AUTOCONFIRM}
      # Uncomment to enable custom access token hook. Please see: https://supabase.com/docs/guides/auth/auth-hooks for full list of hooks and additional details about custom_access_token_hook

      # GOTRUE_HOOK_CUSTOM_ACCESS_TOKEN_ENABLED: "true"
      # GOTRUE_HOOK_CUSTOM_ACCESS_TOKEN_URI: "pg-functions://postgres/public/custom_access_token_hook"
      # GOTRUE_HOOK_CUSTOM_ACCESS_TOKEN_SECRETS: "<standard-base64-secret>"

      # GOTRUE_HOOK_MFA_VERIFICATION_ATTEMPT_ENABLED: "true"
      # GOTRUE_HOOK_MFA_VERIFICATION_ATTEMPT_URI: "pg-functions://postgres/public/mfa_verification_attempt"

      # GOTRUE_HOOK_PASSWORD_VERIFICATION_ATTEMPT_ENABLED: "true"
      # GOTRUE_HOOK_PASSWORD_VERIFICATION_ATTEMPT_URI: "pg-functions://postgres/public/password_verification_attempt"

      # GOTRUE_HOOK_SEND_SMS_ENABLED: "false"
      # GOTRUE_HOOK_SEND_SMS_URI: "pg-functions://postgres/public/custom_access_token_hook"
      # GOTRUE_HOOK_SEND_SMS_SECRETS: "v1,whsec_VGhpcyBpcyBhbiBleGFtcGxlIG9mIGEgc2hvcnRlciBCYXNlNjQgc3RyaW5n"

      # GOTRUE_HOOK_SEND_EMAIL_ENABLED: "false"
      # GOTRUE_HOOK_SEND_EMAIL_URI: "http://host.docker.internal:54321/functions/v1/email_sender"
      # GOTRUE_HOOK_SEND_EMAIL_SECRETS: "v1,whsec_VGhpcyBpcyBhbiBleGFtcGxlIG9mIGEgc2hvcnRlciBCYXNlNjQgc3RyaW5n"

      GOTRUE_EXTERNAL_GOOGLE_ENABLED: "true"
      GOTRUE_EXTERNAL_GOOGLE_CLIENT_ID: ${GOOGLE_CLIENT_ID}
      GOTRUE_EXTERNAL_GOOGLE_SECRET: ${GOOGLE_CLIENT_SECRET}
      GOTRUE_EXTERNAL_GOOGLE_REDIRECT_URI: "https://apibase.ecomweb.app/auth/v1/callback"

      GOTRUE_MAILER_SUBJECTS_CONFIRMATION: "Xác nhận tài khoản"
      GOTRUE_MAILER_TEMPLATES_CONFIRMATION: "http://auth-email-nginx:80/confirm.html"
      GOTRUE_MAILER_SUBJECTS_RECOVERY: "Khôi phục mật khẩu"
      GOTRUE_MAILER_TEMPLATES_RECOVERY: "http://auth-email-nginx:80/recovery.html"
      GOTRUE_MAILER_SUBJECTS_EMAIL_CHANGE: "Thay đổi địa chỉ email"
      GOTRUE_MAILER_TEMPLATES_EMAIL_CHANGE: "http://auth-email-nginx:80/email_change.html"
      GOTRUE_MAILER_SUBJECTS_INVITE: "Bạn đã được mời tham gia EcomWeb"
      GOTRUE_MAILER_TEMPLATES_INVITE: "http://auth-email-nginx:80/invite.html"
      GOTRUE_MAILER_TEMPLATES_MAGIC_LINK: "http://auth-email-nginx:80/magic_link.html"
      GOTRUE_MAILER_SUBJECTS_MAGIC_LINK: "Đăng nhập"

    networks:
      - ecomweb_internal_net

  # Add nginx to serve static html
  auth-email-nginx:
    container_name: supabase-nginx-email
    image: nginx:alpine
    restart: unless-stopped
    volumes:
      - ./volumes/emails:/usr/share/nginx/html:ro # Map your external static folder to the container
    networks:
      - ecomweb_internal_net # Ensure this matches your existing network setup
      # you can check the networks with `docker network ls` 
      # and see whats running within a network with `docker network inspect {network-name}`    

  rest:
    container_name: supabase-rest
    image: postgrest/postgrest:v12.2.12
    restart: unless-stopped
    depends_on:
      db:
        # Disable this if you are using an external Postgres database
        condition: service_healthy
      analytics:
        condition: service_healthy
    environment:
      PGRST_DB_URI: postgres://authenticator:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}
      PGRST_DB_SCHEMAS: ${PGRST_DB_SCHEMAS}
      PGRST_DB_ANON_ROLE: anon
      PGRST_JWT_SECRET: ${JWT_SECRET}
      PGRST_DB_USE_LEGACY_GUCS: "false"
      PGRST_APP_SETTINGS_JWT_SECRET: ${JWT_SECRET}
      PGRST_APP_SETTINGS_JWT_EXP: ${JWT_EXPIRY}
    command:
      [
        "postgrest"
      ]

    networks:
      - ecomweb_internal_net  

  realtime:
    # This container name looks inconsistent but is correct because realtime constructs tenant id by parsing the subdomain
    container_name: realtime-dev.supabase-realtime
    image: supabase/realtime:v2.34.47
    restart: unless-stopped
    depends_on:
      db:
        # Disable this if you are using an external Postgres database
        condition: service_healthy
      analytics:
        condition: service_healthy
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "-sSfL",
          "--head",
          "-o",
          "/dev/null",
          "-H",
          "Authorization: Bearer ${ANON_KEY}",
          "http://localhost:4000/api/tenants/realtime-dev/health"
        ]
      timeout: 5s
      interval: 5s
      retries: 3
    environment:
      PORT: 4000
      DB_HOST: ${POSTGRES_HOST}
      DB_PORT: ${POSTGRES_PORT}
      DB_USER: supabase_admin
      DB_PASSWORD: ${POSTGRES_PASSWORD}
      DB_NAME: ${POSTGRES_DB}
      DB_AFTER_CONNECT_QUERY: 'SET search_path TO _realtime'
      DB_ENC_KEY: supabaserealtime
      API_JWT_SECRET: ${JWT_SECRET}
      SECRET_KEY_BASE: ${SECRET_KEY_BASE}
      ERL_AFLAGS: -proto_dist inet_tcp
      DNS_NODES: "''"
      RLIMIT_NOFILE: "10000"
      APP_NAME: realtime
      SEED_SELF_HOST: true
      RUN_JANITOR: true
    
    networks:
      - ecomweb_internal_net  

  # To use S3 backed storage: docker compose -f docker-compose.yml -f docker-compose.s3.yml up
  # storage:
  #   container_name: supabase-storage
  #   image: supabase/storage-api:v1.25.7
  #   restart: unless-stopped
  #   volumes:
  #     - ./volumes/storage:/var/lib/storage:z
  #   healthcheck:
  #     test:
  #       [
  #         "CMD",
  #         "wget",
  #         "--no-verbose",
  #         "--tries=1",
  #         "--spider",
  #         "http://storage:5000/status"
  #       ]
  #     timeout: 5s
  #     interval: 5s
  #     retries: 3
  #   depends_on:
  #     db:
  #       # Disable this if you are using an external Postgres database
  #       condition: service_healthy
  #     rest:
  #       condition: service_started
  #     imgproxy:
  #       condition: service_started
  #   environment:
  #     ANON_KEY: ${ANON_KEY}
  #     SERVICE_KEY: ${SERVICE_ROLE_KEY}
  #     POSTGREST_URL: http://rest:3000
  #     PGRST_JWT_SECRET: ${JWT_SECRET}
  #     DATABASE_URL: postgres://supabase_storage_admin:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}
  #     FILE_SIZE_LIMIT: 52428800
  #     STORAGE_BACKEND: file
  #     FILE_STORAGE_BACKEND_PATH: /var/lib/storage
  #     TENANT_ID: stub
  #     # TODO: https://github.com/supabase/storage-api/issues/55
  #     REGION: stub
  #     GLOBAL_S3_BUCKET: stub
  #     ENABLE_IMAGE_TRANSFORMATION: "true"
  #     IMGPROXY_URL: http://imgproxy:5001

  #   networks:
  #     - ecomweb_internal_net  

  # imgproxy:
  #   container_name: supabase-imgproxy
  #   image: darthsim/imgproxy:v3.8.0
  #   restart: unless-stopped
  #   volumes:
  #     - ./volumes/storage:/var/lib/storage:z
  #   healthcheck:
  #     test:
  #       [
  #         "CMD",
  #         "imgproxy",
  #         "health"
  #       ]
  #     timeout: 5s
  #     interval: 5s
  #     retries: 3
  #   environment:
  #     IMGPROXY_BIND: ":5001"
  #     IMGPROXY_LOCAL_FILESYSTEM_ROOT: /
  #     IMGPROXY_USE_ETAG: "true"
  #     IMGPROXY_ENABLE_WEBP_DETECTION: ${IMGPROXY_ENABLE_WEBP_DETECTION}
    
  #   networks:
  #     - ecomweb_internal_net

  minio-ecomapp:
    image: minio/minio
    restart: unless-stopped
    ports:
      - '9000'
      - '9001'
    environment:
      MINIO_ROOT_USER: supa-storage
      MINIO_ROOT_PASSWORD: secret1234
    command: server --console-address ":9001" /data
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://minio-ecomapp:9000/minio/health/live" ]
      interval: 2s
      timeout: 10s
      retries: 5
    volumes:
      - ./volumes/storage:/data:z

    networks:
      - ecomweb_internal_net

  minio-createbucket:
    image: minio/mc
    depends_on:
      minio-ecomapp:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc alias set supa-minio http://minio-ecomapp:9000 supa-storage secret1234;
      /usr/bin/mc mb supa-minio/stub;
      exit 0;
      "

    networks:
      - ecomweb_internal_net  

  storage:
    container_name: supabase-storage
    image: supabase/storage-api:v1.11.13
    depends_on:
      db:
        # Disable this if you are using an external Postgres database
        condition: service_healthy
      rest:
        condition: service_started
      imgproxy:
        condition: service_started
      minio-ecomapp:
        condition: service_healthy
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://localhost:5000/status"
        ]
      timeout: 5s
      interval: 5s
      retries: 3
    restart: unless-stopped
    environment:
      ANON_KEY: ${ANON_KEY}
      SERVICE_KEY: ${SERVICE_ROLE_KEY}
      POSTGREST_URL: http://rest:3000
      PGRST_JWT_SECRET: ${JWT_SECRET}
      DATABASE_URL: postgres://supabase_storage_admin:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}
      FILE_SIZE_LIMIT: 52428800
      STORAGE_BACKEND: s3
      GLOBAL_S3_BUCKET: stub
      GLOBAL_S3_ENDPOINT: http://minio-ecomapp:9000
      GLOBAL_S3_PROTOCOL: http
      GLOBAL_S3_FORCE_PATH_STYLE: true
      AWS_ACCESS_KEY_ID: supa-storage
      AWS_SECRET_ACCESS_KEY: secret1234
      AWS_DEFAULT_REGION: stub
      FILE_STORAGE_BACKEND_PATH: /var/lib/storage
      TENANT_ID: stub
      # TODO: https://github.com/supabase/storage-api/issues/55
      REGION: stub
      ENABLE_IMAGE_TRANSFORMATION: "true"
      IMGPROXY_URL: http://imgproxy:5001
    volumes:
      - ./volumes/storage:/var/lib/storage:z

    networks:
      - ecomweb_internal_net  

  imgproxy:
    container_name: supabase-imgproxy
    image: darthsim/imgproxy:v3.8.0
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "imgproxy", "health" ]
      timeout: 5s
      interval: 5s
      retries: 3
    environment:
      IMGPROXY_BIND: ":5001"
      IMGPROXY_USE_ETAG: "true"
      IMGPROXY_ENABLE_WEBP_DETECTION: ${IMGPROXY_ENABLE_WEBP_DETECTION}

    networks:
      - ecomweb_internal_net
  meta:
    container_name: supabase-meta
    image: supabase/postgres-meta:v0.91.0
    restart: unless-stopped
    depends_on:
      db:
        # Disable this if you are using an external Postgres database
        condition: service_healthy
      analytics:
        condition: service_healthy
    environment:
      PG_META_PORT: 8080
      PG_META_DB_HOST: ${POSTGRES_HOST}
      PG_META_DB_PORT: ${POSTGRES_PORT}
      PG_META_DB_NAME: ${POSTGRES_DB}
      PG_META_DB_USER: supabase_admin
      PG_META_DB_PASSWORD: ${POSTGRES_PASSWORD}

    networks:
      - ecomweb_internal_net

  functions:
    container_name: supabase-edge-functions
    image: supabase/edge-runtime:v1.67.4
    restart: unless-stopped
    volumes:
      - ./volumes/functions:/home/deno/functions:Z
    depends_on:
      analytics:
        condition: service_healthy
    environment:
      JWT_SECRET: ${JWT_SECRET}
      SUPABASE_URL: http://kong:8000
      SUPABASE_ANON_KEY: ${ANON_KEY}
      SUPABASE_SERVICE_ROLE_KEY: ${SERVICE_ROLE_KEY}
      SUPABASE_DB_URL: postgresql://postgres:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}
      # TODO: Allow configuring VERIFY_JWT per function. This PR might help: https://github.com/supabase/cli/pull/786
      VERIFY_JWT: "${FUNCTIONS_VERIFY_JWT}"
    command:
      [
        "start",
        "--main-service",
        "/home/deno/functions/main"
      ]

    networks:
      - ecomweb_internal_net  

  analytics:
    container_name: supabase-analytics
    image: supabase/logflare:1.14.2
    restart: unless-stopped
    ports:
      - 10.7.0.4:4000:4000
    # Uncomment to use Big Query backend for analytics
    volumes:
      - type: bind
        source: ${PWD}/gcloud.json
        target: /opt/app/rel/logflare/bin/gcloud.json
        read_only: true
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "http://localhost:4000/health"
        ]
      timeout: 5s
      interval: 5s
      retries: 10
    depends_on:
      db:
        # Disable this if you are using an external Postgres database
        condition: service_healthy
    environment:
      LOGFLARE_NODE_HOST: 127.0.0.1
      DB_USERNAME: supabase_admin
      DB_DATABASE: _supabase
      DB_HOSTNAME: ${POSTGRES_HOST}
      DB_PORT: ${POSTGRES_PORT}
      DB_PASSWORD: ${POSTGRES_PASSWORD}
      DB_SCHEMA: _analytics
      LOGFLARE_PUBLIC_ACCESS_TOKEN: ${LOGFLARE_PUBLIC_ACCESS_TOKEN}
      LOGFLARE_PRIVATE_ACCESS_TOKEN: ${LOGFLARE_PRIVATE_ACCESS_TOKEN}
      LOGFLARE_SINGLE_TENANT: true
      LOGFLARE_SUPABASE_MODE: true
      LOGFLARE_MIN_CLUSTER_SIZE: 1

      # Comment variables to use Big Query backend for analytics
      # POSTGRES_BACKEND_URL: postgresql://supabase_admin:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/_supabase
      # POSTGRES_BACKEND_SCHEMA: _analytics
      LOGFLARE_FEATURE_FLAG_OVERRIDE: multibackend=true
      # Uncomment to use Big Query backend for analytics
      GOOGLE_PROJECT_ID: ${GOOGLE_PROJECT_ID}
      GOOGLE_PROJECT_NUMBER: ${GOOGLE_PROJECT_NUMBER}

    networks:
      - ecomweb_internal_net
  # Comment out everything below this point if you are using an external Postgres database
  db:
    container_name: supabase-db
    image: supabase/postgres:15.8.1.060
    restart: unless-stopped
    volumes:
      - ./volumes/db/realtime.sql:/docker-entrypoint-initdb.d/migrations/99-realtime.sql:Z
      # Must be superuser to create event trigger
      - ./volumes/db/webhooks.sql:/docker-entrypoint-initdb.d/init-scripts/98-webhooks.sql:Z
      # Replication setup - configure pg_hba.conf for replication connections
      - ./volumes/db/replication/configure-pg-hba.sh:/docker-entrypoint-initdb.d/init-scripts/96-configure-pg-hba.sh:Z
      # Replication setup - create replicator user and slots
      - ./volumes/db/replication/init-primary-replication.sql:/docker-entrypoint-initdb.d/init-scripts/97-replication.sql:Z
      # Must be superuser to alter reserved role
      - ./volumes/db/roles.sql:/docker-entrypoint-initdb.d/init-scripts/99-roles.sql:Z
      # Initialize the database settings with JWT_SECRET and JWT_EXP
      - ./volumes/db/jwt.sql:/docker-entrypoint-initdb.d/init-scripts/99-jwt.sql:Z
      # PGDATA directory is persisted between restarts
      - ./volumes/db/data:/var/lib/postgresql/data:Z
      # Changes required for internal supabase data such as _analytics
      - ./volumes/db/_supabase.sql:/docker-entrypoint-initdb.d/migrations/97-_supabase.sql:Z
      # Changes required for Analytics support
      - ./volumes/db/logs.sql:/docker-entrypoint-initdb.d/migrations/99-logs.sql:Z
      # Changes required for Pooler support
      - ./volumes/db/pooler.sql:/docker-entrypoint-initdb.d/migrations/99-pooler.sql:Z
      # Use named volume to persist pgsodium decryption key between restarts
      - db-config:/etc/postgresql-custom
    healthcheck:
      test:
        [
        "CMD",
        "pg_isready",
        "-U",
        "postgres",
        "-h",
        "localhost"
        ]
      interval: 5s
      timeout: 5s
      retries: 10
    depends_on:
      vector:
        condition: service_healthy
    environment:
      POSTGRES_HOST: /var/run/postgresql
      PGPORT: ${POSTGRES_PORT}
      POSTGRES_PORT: ${POSTGRES_PORT}
      PGPASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      PGDATABASE: ${POSTGRES_DB}
      POSTGRES_DB: ${POSTGRES_DB}
      JWT_SECRET: ${JWT_SECRET}
      JWT_EXP: ${JWT_EXPIRY}
    command:
      [
        "postgres",
        "-c",
        "config_file=/etc/postgresql/postgresql.conf",
        "-c",
        "log_min_messages=fatal" # prevents Realtime polling queries from appearing in logs
      ]

    networks:
      - ecomweb_internal_net

  # Database Backup Service - Automated scheduled backups
  db-backup:
    image: ghcr.io/travistech20/ecomweb-db-backup:production
    container_name: supabase-db-backup
    restart: unless-stopped
    depends_on:
      db:
        condition: service_healthy
    environment:
      DB_CONTAINER: supabase-db
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      BACKUP_SCHEDULE: ${BACKUP_SCHEDULE:-0 2 * * *}  # Default: 2 AM daily
      MAX_BACKUPS: ${MAX_BACKUPS:-7}  # Keep last 7 backups
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro  # Access to Docker daemon
      - ./volumes/backups/scheduled:/backups:z  # Backup storage
      - ./scripts/backup:/scripts:ro  # Backup scripts (optional, for reference)
    networks:
      - ecomweb_internal_net

  # PostgreSQL Read Replica 1 - Streaming Replication
  db-replica-1:
    container_name: supabase-db-replica-1
    image: supabase/postgres:15.8.1.060
    restart: unless-stopped
    ports:
      - 54325:5432
    volumes:
      - db-replica-1-data:/var/lib/postgresql/data
      - db-replica-1-config:/etc/postgresql-custom
      - ./volumes/db/replication/init-replica.sh:/init-replica.sh:ro
    healthcheck:
      test:
        [
        "CMD",
        "pg_isready",
        "-U",
        "postgres",
        "-h",
        "localhost"
        ]
      interval: 10s
      timeout: 5s
      retries: 10
    depends_on:
      db:
        condition: service_healthy
    environment:
      POSTGRES_HOST: /var/run/postgresql
      PGPORT: ${POSTGRES_PORT}
      POSTGRES_PORT: ${POSTGRES_PORT}
      PGPASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      PGDATABASE: ${POSTGRES_DB}
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: postgres
      PRIMARY_HOST: db
      PRIMARY_PORT: ${POSTGRES_PORT}
      REPLICATION_USER: replicator
      REPLICATION_PASSWORD: ${REPLICATION_PASSWORD:-replicator_password}
      PGDATA: /var/lib/postgresql/data
      REPLICA_SLOT_NAME: replica_1_slot
    entrypoint: ["/bin/bash", "/init-replica.sh"]
    networks:
      - ecomweb_internal_net

  vector:
    container_name: supabase-vector
    image: timberio/vector:0.28.1-alpine
    restart: unless-stopped
    volumes:
      - ./volumes/logs/vector.yml:/etc/vector/vector.yml:ro,z
      - ${DOCKER_SOCKET_LOCATION}:/var/run/docker.sock:ro,z
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://vector:9001/health"
        ]
      timeout: 5s
      interval: 5s
      retries: 3
    environment:
      LOGFLARE_PUBLIC_ACCESS_TOKEN: ${LOGFLARE_PUBLIC_ACCESS_TOKEN}
    command:
      [
        "--config",
        "/etc/vector/vector.yml"
      ]
    security_opt:
      - "label=disable"

    networks:
      - ecomweb_internal_net  

  # Update the DATABASE_URL if you are using an external Postgres database
  supavisor:
    container_name: supabase-pooler
    image: supabase/supavisor:2.5.7
    restart: unless-stopped
    ports:
      - 10.7.0.4:5433:5432
      - ${POOLER_PROXY_PORT_TRANSACTION}:6543
    volumes:
      - ./volumes/pooler/pooler.exs:/etc/pooler/pooler.exs:ro,z
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "-sSfL",
          "--head",
          "-o",
          "/dev/null",
          "http://127.0.0.1:4000/api/health"
        ]
      interval: 10s
      timeout: 5s
      retries: 5
    depends_on:
      db:
        condition: service_healthy
      analytics:
        condition: service_healthy
    environment:
      PORT: 4000
      POSTGRES_PORT: ${POSTGRES_PORT}
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      DATABASE_URL: ecto://supabase_admin:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/_supabase
      CLUSTER_POSTGRES: true
      SECRET_KEY_BASE: ${SECRET_KEY_BASE}
      VAULT_ENC_KEY: ${VAULT_ENC_KEY}
      API_JWT_SECRET: ${JWT_SECRET}
      METRICS_JWT_SECRET: ${JWT_SECRET}
      REGION: local
      ERL_AFLAGS: -proto_dist inet_tcp
      POOLER_TENANT_ID: ${POOLER_TENANT_ID}
      POOLER_DEFAULT_POOL_SIZE: ${POOLER_DEFAULT_POOL_SIZE}
      POOLER_MAX_CLIENT_CONN: ${POOLER_MAX_CLIENT_CONN}
      POOLER_POOL_MODE: transaction
      DB_POOL_SIZE: ${POOLER_DB_POOL_SIZE}
    command:
      [
        "/bin/sh",
        "-c",
        "/app/bin/migrate && /app/bin/supavisor eval \"$$(cat /etc/pooler/pooler.exs)\" && /app/bin/server"
      ]

    networks:
      - ecomweb_internal_net

  typesense:
    image: typesense/typesense:29.0
    restart: unless-stopped
    container_name: ecomweb-typesense
    ports:
      - "10.7.0.4:8108:8108"
    volumes:
      - typesense-data:/data
    command: >
      --data-dir /data
      --api-key=${TYPESENSE_API_KEY:-dev-api-key-12345}
      --enable-cors
      --log-level=info
    environment:
      - TYPESENSE_API_KEY=${TYPESENSE_API_KEY:-dev-api-key-12345}
    networks:
      - ecomweb_internal_net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8108/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # sequin:
  #   image: sequin/sequin:latest
  #   ports:
  #     - "10.7.0.4:7376:7376"
  #   environment:
  #     - PG_HOSTNAME=sequin_postgres
  #     - PG_DATABASE=sequin
  #     - PG_PORT=5432
  #     - PG_USERNAME=postgres
  #     - PG_PASSWORD=postgres
  #     - PG_POOL_SIZE=20
  #     # Un-comment for SSL
  #     # - PG_SSL=true
  #     - SECRET_KEY_BASE=wDPLYus0pvD6qJhKJICO4dauYPXfO/Yl782Zjtpew5qRBDp7CZvbWtQmY0eB13If
  #     - VAULT_KEY=2Sig69bIpuSm2kv0VQfDekET2qy8qUZGI8v3/h3ASiY=
  #     - REDIS_URL=redis://sequin_redis:6379
  #     - CONFIG_FILE_PATH=/config/sequin.yaml
  #   volumes:
  #       - ./volumes/sequin/sequin.yaml:/config/sequin.yaml
  #   depends_on:
  #     sequin_redis:
  #       condition: service_started
  #     sequin_postgres:
  #       condition: service_healthy
  #   networks:
  #     - ecomweb_internal_net

  # sequin_postgres:
  #   image: postgres:16
  #   ports:
  #     - "5432"
  #   environment:
  #     - POSTGRES_DB=sequin
  #     - POSTGRES_USER=postgres
  #     - POSTGRES_PASSWORD=postgres
  #   command: ["postgres", "-c", "wal_level=logical"]
  #   volumes:
  #     - sequin_postgres_data:/var/lib/postgresql/data
  #     # Creates a sample database for Sequin's Quickstart guide
  #     - ./postgres-init:/docker-entrypoint-initdb.d
  #   healthcheck:
  #     test: ["CMD-SHELL", "pg_isready -U postgres -d sequin"]
  #     interval: 10s
  #     timeout: 2s
  #     retries: 5
  #     start_period: 2s
  #     start_interval: 1s
  #   networks:
  #     - ecomweb_internal_net

  # sequin_redis:
  #   image: redis:7
  #   ports:
  #     - "7378:6379"
  #   command: ["redis-server", "--port", "6379"]
  #   volumes:
  #     - sequin_redis_data:/data
  #   networks:
  #     - ecomweb_internal_net

  # Redis for BullMQ (Job Queue)
  bullmq_redis:
    image: redis:7-alpine
    container_name: ecomweb-bullmq-redis
    restart: unless-stopped
    ports:
      - "6379"
    command: >
      redis-server
      --appendonly yes
      --appendfsync everysec
      --maxmemory 256mb
      --maxmemory-policy noeviction
      --save 60 1000
    volumes:
      - bullmq_redis_data:/data
    networks:
      - ecomweb_internal_net
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 5s

  # Bull Board UI for Queue Monitoring
  bullboard:
    image: venatum/bull-board:latest
    container_name: ecomweb-bullboard
    restart: unless-stopped
    ports:
      - "3000"
    environment:
      - REDIS_HOST=bullmq_redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=
      - REDIS_USE_TLS=false
      - BULL_PREFIX=bull
    depends_on:
      bullmq_redis:
        condition: service_healthy
    networks:
      - ecomweb_internal_net

  # ============================================
  # Store Supabase Instance (Auth-Only) - Minimal Setup
  # ============================================

  # PostgreSQL Database for Store Auth
  store-db:
    container_name: store-supabase-db
    image: supabase/postgres:15.8.1.060
    restart: unless-stopped
    ports:
      - 54323:5432
    volumes:
      - ./volumes/db/realtime.sql:/docker-entrypoint-initdb.d/migrations/99-realtime.sql:Z
      - ./volumes/db/webhooks.sql:/docker-entrypoint-initdb.d/init-scripts/98-webhooks.sql:Z
      - ./volumes/db/roles.sql:/docker-entrypoint-initdb.d/init-scripts/99-roles.sql:Z
      - ./volumes/db/jwt.sql:/docker-entrypoint-initdb.d/init-scripts/99-jwt.sql:Z
      - ./volumes/store-db/data:/var/lib/postgresql/data:Z
      - ./volumes/db/_supabase.sql:/docker-entrypoint-initdb.d/migrations/97-_supabase.sql:Z
      - ./volumes/db/logs.sql:/docker-entrypoint-initdb.d/migrations/99-logs.sql:Z
      - store-db-config:/etc/postgresql-custom
    healthcheck:
      test:
        [
        "CMD",
        "pg_isready",
        "-U",
        "postgres",
        "-h",
        "localhost"
        ]
      interval: 5s
      timeout: 5s
      retries: 10
    environment:
      POSTGRES_HOST: /var/run/postgresql
      PGPORT: ${POSTGRES_PORT}
      POSTGRES_PORT: ${POSTGRES_PORT}
      PGPASSWORD: ${STORE_POSTGRES_PASSWORD}
      POSTGRES_PASSWORD: ${STORE_POSTGRES_PASSWORD}
      PGDATABASE: ${POSTGRES_DB}
      POSTGRES_DB: ${POSTGRES_DB}
      JWT_SECRET: ${STORE_JWT_SECRET}
      JWT_EXP: ${JWT_EXPIRY}
    command:
      [
        "postgres",
        "-c",
        "config_file=/etc/postgresql/postgresql.conf",
        "-c",
        "log_min_messages=fatal"
      ]
    networks:
      - ecomweb_internal_net

  # Database Backup Service for Store DB
  store-db-backup:
    image: ghcr.io/travistech20/ecomweb-db-backup:production
    container_name: store-supabase-db-backup
    restart: unless-stopped
    depends_on:
      store-db:
        condition: service_healthy
    environment:
      DB_CONTAINER: store-supabase-db
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${STORE_POSTGRES_PASSWORD}
      BACKUP_SCHEDULE: ${STORE_BACKUP_SCHEDULE:-0 3 * * *}  # Default: 3 AM daily (offset from main db)
      MAX_BACKUPS: ${STORE_MAX_BACKUPS:-7}  # Keep last 7 backups
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro  # Access to Docker daemon
      - ./volumes/backups/store-scheduled:/backups:z  # Backup storage
      - ./scripts/backup:/scripts:ro  # Backup scripts (optional, for reference)
    networks:
      - ecomweb_internal_net

  # Analytics service for Store Auth
  store-analytics:
    container_name: store-supabase-analytics
    image: supabase/logflare:1.14.2
    restart: unless-stopped
    ports:
      - 4002:4000
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "http://localhost:4000/health"
        ]
      timeout: 5s
      interval: 5s
      retries: 10
    depends_on:
      store-db:
        condition: service_healthy
    environment:
      LOGFLARE_NODE_HOST: 127.0.0.1
      DB_USERNAME: supabase_admin
      DB_DATABASE: _supabase
      DB_HOSTNAME: ${STORE_POSTGRES_HOST}
      DB_PORT: ${POSTGRES_PORT}
      DB_PASSWORD: ${STORE_POSTGRES_PASSWORD}
      DB_SCHEMA: _analytics
      LOGFLARE_PUBLIC_ACCESS_TOKEN: ${STORE_LOGFLARE_PUBLIC_ACCESS_TOKEN}
      LOGFLARE_PRIVATE_ACCESS_TOKEN: ${STORE_LOGFLARE_PRIVATE_ACCESS_TOKEN}
      LOGFLARE_SINGLE_TENANT: true
      LOGFLARE_SUPABASE_MODE: true
      LOGFLARE_MIN_CLUSTER_SIZE: 1
      POSTGRES_BACKEND_URL: postgresql://supabase_admin:${STORE_POSTGRES_PASSWORD}@${STORE_POSTGRES_HOST}:${POSTGRES_PORT}/_supabase
      POSTGRES_BACKEND_SCHEMA: _analytics
      LOGFLARE_FEATURE_FLAG_OVERRIDE: multibackend=true
    networks:
      - ecomweb_internal_net

  store-auth-email-nginx:
    container_name: store-supabase-nginx-email
    image: nginx:alpine
    restart: unless-stopped
    volumes:
      - ./volumes/store-emails:/usr/share/nginx/html:ro # Map your external static folder to the container
    networks:
      - ecomweb_internal_net # Ensure this matches your existing network setup

  # GoTrue Auth Service for Stores
  store-auth:
    container_name: store-supabase-auth
    image: supabase/gotrue:v2.177.0
    restart: unless-stopped
    dns:
      - 8.8.8.8
      - 1.1.1.1
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://localhost:9999/health"
        ]
      timeout: 5s
      interval: 5s
      retries: 3
    depends_on:
      store-db:
        condition: service_healthy
      store-analytics:
        condition: service_healthy
    environment:
      GOTRUE_API_HOST: 0.0.0.0
      GOTRUE_API_PORT: 9999
      API_EXTERNAL_URL: ${STORE_API_EXTERNAL_URL}

      GOTRUE_DB_DRIVER: postgres
      GOTRUE_DB_DATABASE_URL: postgres://supabase_auth_admin:${STORE_POSTGRES_PASSWORD}@${STORE_POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}

      GOTRUE_SITE_URL: ${STORE_SITE_URL}
      GOTRUE_URI_ALLOW_LIST: ${STORE_ADDITIONAL_REDIRECT_URLS}
      GOTRUE_DISABLE_SIGNUP: ${DISABLE_SIGNUP}

      GOTRUE_JWT_ADMIN_ROLES: service_role
      GOTRUE_JWT_AUD: authenticated
      GOTRUE_JWT_DEFAULT_GROUP_NAME: authenticated
      GOTRUE_JWT_EXP: ${JWT_EXPIRY}
      GOTRUE_JWT_SECRET: ${STORE_JWT_SECRET}

      GOTRUE_EXTERNAL_EMAIL_ENABLED: ${ENABLE_EMAIL_SIGNUP}
      GOTRUE_EXTERNAL_ANONYMOUS_USERS_ENABLED: ${ENABLE_ANONYMOUS_USERS}
      GOTRUE_MAILER_AUTOCONFIRM: ${ENABLE_EMAIL_AUTOCONFIRM}

      GOTRUE_EXTERNAL_SKIP_NONCE_CHECK: true

      GOTRUE_SMTP_ADMIN_EMAIL: ${SMTP_ADMIN_EMAIL}
      GOTRUE_SMTP_HOST: ${SMTP_HOST}
      GOTRUE_SMTP_PORT: ${SMTP_PORT}
      GOTRUE_SMTP_USER: ${SMTP_USER}
      GOTRUE_SMTP_PASS: ${SMTP_PASS}
      GOTRUE_SMTP_SENDER_NAME: ${SMTP_SENDER_NAME}
      GOTRUE_MAILER_URLPATHS_INVITE: ${MAILER_URLPATHS_INVITE}
      GOTRUE_MAILER_URLPATHS_CONFIRMATION: ${MAILER_URLPATHS_CONFIRMATION}
      GOTRUE_MAILER_URLPATHS_RECOVERY: ${MAILER_URLPATHS_RECOVERY}
      GOTRUE_MAILER_URLPATHS_EMAIL_CHANGE: ${MAILER_URLPATHS_EMAIL_CHANGE}

      GOTRUE_EXTERNAL_PHONE_ENABLED: ${ENABLE_PHONE_SIGNUP}
      GOTRUE_SMS_AUTOCONFIRM: ${ENABLE_PHONE_AUTOCONFIRM}

      # OAuth Providers
      GOTRUE_EXTERNAL_GOOGLE_ENABLED: "true"
      GOTRUE_EXTERNAL_GOOGLE_CLIENT_ID: ${GOOGLE_CLIENT_ID}
      GOTRUE_EXTERNAL_GOOGLE_SECRET: ${GOOGLE_CLIENT_SECRET}
      GOTRUE_EXTERNAL_GOOGLE_REDIRECT_URI: "https://apisite.ecomweb.app/auth/v1/callback"

      # Email Templates (Vietnamese)
      GOTRUE_MAILER_SUBJECTS_CONFIRMATION: "Xác nhận tài khoản"
      GOTRUE_MAILER_TEMPLATES_CONFIRMATION: "http://store-auth-email-nginx:80/confirm.html"
      GOTRUE_MAILER_SUBJECTS_RECOVERY: "Khôi phục mật khẩu"
      GOTRUE_MAILER_TEMPLATES_RECOVERY: "http://store-auth-email-nginx:80/recovery.html"
      GOTRUE_MAILER_SUBJECTS_EMAIL_CHANGE: "Thay đổi địa chỉ email"
      GOTRUE_MAILER_TEMPLATES_EMAIL_CHANGE: "http://store-auth-email-nginx:80/email_change.html"
      GOTRUE_MAILER_SUBJECTS_INVITE: "Bạn đã được mời tham gia EcomWeb Store"
      GOTRUE_MAILER_TEMPLATES_INVITE: "http://store-auth-email-nginx:80/invite.html"
      GOTRUE_MAILER_TEMPLATES_MAGIC_LINK: "http://store-auth-email-nginx:80/magic_link.html"
      GOTRUE_MAILER_SUBJECTS_MAGIC_LINK: "Đăng nhập"

    networks:
      - ecomweb_internal_net

  # Kong API Gateway for Store Auth
  store-kong:
    container_name: store-supabase-kong
    image: kong:2.8.1
    restart: unless-stopped
    ports:
      - ${STORE_KONG_HTTP_PORT}:8000/tcp
      - ${STORE_KONG_HTTPS_PORT}:8443/tcp
    volumes:
      - ./volumes/api/kong-store.yml:/home/kong/temp.yml:ro,z
    depends_on:
      # store-analytics:
      #   condition: service_healthy
      store-auth:
        condition: service_healthy
    environment:
      KONG_DATABASE: "off"
      KONG_DECLARATIVE_CONFIG: /home/kong/kong.yml
      KONG_DNS_ORDER: LAST,A,CNAME
      KONG_PLUGINS: request-transformer,cors,key-auth,acl,basic-auth
      KONG_NGINX_PROXY_PROXY_BUFFER_SIZE: 160k
      KONG_NGINX_PROXY_PROXY_BUFFERS: 64 160k
      SUPABASE_ANON_KEY: ${STORE_ANON_KEY}
      SUPABASE_SERVICE_KEY: ${STORE_SERVICE_ROLE_KEY}
      DASHBOARD_USERNAME: ${STORE_DASHBOARD_USERNAME}
      DASHBOARD_PASSWORD: ${STORE_DASHBOARD_PASSWORD}
    entrypoint: bash -c 'eval "echo \"$$(cat ~/temp.yml)\"" > ~/kong.yml && /docker-entrypoint.sh kong docker-start'
    networks:
      - ecomweb_internal_net
      - ecomweb_external_net

  # Postgres Meta for Store Auth
  store-meta:
    container_name: store-supabase-meta
    image: supabase/postgres-meta:v0.91.0
    restart: unless-stopped
    depends_on:
      store-db:
        condition: service_healthy
      # store-analytics:
      #   condition: service_healthy
    environment:
      PG_META_PORT: 8080
      PG_META_DB_HOST: ${STORE_POSTGRES_HOST}
      PG_META_DB_PORT: ${POSTGRES_PORT}
      PG_META_DB_NAME: ${POSTGRES_DB}
      PG_META_DB_USER: supabase_admin
      PG_META_DB_PASSWORD: ${STORE_POSTGRES_PASSWORD}
    networks:
      - ecomweb_internal_net

  # Supabase Studio for Store Auth
  store-studio:
    container_name: store-supabase-studio
    image: supabase/studio:2025.06.30-sha-6f5982d
    restart: unless-stopped
    ports:
      - ${STORE_STUDIO_PORT}:3000
    healthcheck:
      test:
        [
          "CMD",
          "node",
          "-e",
          "fetch('http://store-studio:3000/api/platform/profile').then((r) => {if (r.status !== 200) throw new Error(r.status)})"
        ]
      timeout: 10s
      interval: 5s
      retries: 3
    depends_on:
      # store-analytics:
      #   condition: service_healthy
      store-meta:
        condition: service_started
    environment:
      STUDIO_PG_META_URL: http://store-meta:8080
      POSTGRES_PASSWORD: ${STORE_POSTGRES_PASSWORD}

      DEFAULT_ORGANIZATION_NAME: ${STUDIO_DEFAULT_ORGANIZATION}
      DEFAULT_PROJECT_NAME: "Store Auth Instance"

      SUPABASE_URL: http://store-kong:8000
      SUPABASE_PUBLIC_URL: ${STORE_SUPABASE_PUBLIC_URL}
      SUPABASE_ANON_KEY: ${STORE_ANON_KEY}
      SUPABASE_SERVICE_KEY: ${STORE_SERVICE_ROLE_KEY}
      AUTH_JWT_SECRET: ${STORE_JWT_SECRET}

      LOGFLARE_PRIVATE_ACCESS_TOKEN: ${STORE_LOGFLARE_PRIVATE_ACCESS_TOKEN}
      LOGFLARE_URL: http://store-analytics:4000
      NEXT_PUBLIC_ENABLE_LOGS: true
    networks:
      - ecomweb_internal_net

  

  # ============================================
  # Temporal Workflow Engine
  # ============================================

  # PostgreSQL database for Temporal
  temporal-postgres:
    container_name: temporal-postgresql
    image: postgres:13
    restart: unless-stopped
    environment:
      POSTGRES_PASSWORD: temporal
      POSTGRES_USER: temporal
      POSTGRES_DB: temporal
    ports:
      - "54324:5432"
    volumes:
      - temporal-postgres-data:/var/lib/postgresql/data
    networks:
      - ecomweb_internal_net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U temporal"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Temporal Server (combined services)
  temporal:
    container_name: temporal-server
    image: temporalio/auto-setup:1.25.2
    restart: unless-stopped
    depends_on:
      temporal-postgres:
        condition: service_healthy
    environment:
      - DB=postgres12
      - DB_PORT=5432
      - POSTGRES_USER=temporal
      - POSTGRES_PWD=temporal
      - POSTGRES_SEEDS=temporal-postgres
      - DYNAMIC_CONFIG_FILE_PATH=config/dynamicconfig/development-sql.yaml
      - ENABLE_ES=false
      - ES_SEEDS=
      - ES_VERSION=v7
      - TEMPORAL_ADDRESS=temporal:7233
      - TEMPORAL_CLI_ADDRESS=temporal:7233
    ports:
      - "7233:7233"  # gRPC port (for SDK connection)
      - "7234:7234"  # Membership port
      - "7235:7235"  # History port
      - "7239:7239"  # Worker port
    volumes:
      - ./volumes/temporal/config:/etc/temporal/config/dynamicconfig
    networks:
      - ecomweb_internal_net
    healthcheck:
      test: ["CMD", "tctl", "--address", "temporal:7233", "cluster", "health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # Temporal Web UI
  temporal-ui:
    container_name: temporal-ui
    image: temporalio/ui:2.32.0
    restart: unless-stopped
    depends_on:
      temporal:
        condition: service_started
    environment:
      - TEMPORAL_ADDRESS=temporal:7233
      - TEMPORAL_CORS_ORIGINS=http://localhost:3000,http://localhost:3001
    ports:
      - "8088:8080"
    networks:
      - ecomweb_internal_net
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3    

volumes:
  db-config:
  db-replica-1-data:
  db-replica-1-config:
  typesense-data:
  sequin_postgres_data:
  sequin_redis_data:
  temporal-postgres-data:
  store-db-config:
  bullmq_redis_data:
  haproxy-socket:
